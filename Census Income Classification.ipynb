{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML('<style> .container{ width:90%; } </style>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing standard libraries, which only needs to be done once:\n",
    "- Scientific computing (numpy)\n",
    "- Data analysis (pandas)\n",
    "- Plotting (matplotlib, seaborn)\n",
    "- Machine learning (scikit-learn)\n",
    "- Gradient-boosted trees (xgboost. lightgbm)\n",
    "- Deep learning (tensorflow, keras, keras_metrics)\n",
    "\n",
    "Uncomment for first run!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''import sys\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install seaborn\n",
    "!{sys.executable} -m pip install sklearn\n",
    "!{sys.executable} -m pip install xgboost\n",
    "!{sys.executable} -m pip install lightgbm\n",
    "!{sys.executable} -m pip install tensorflow\n",
    "!{sys.executable} -m pip install keras\n",
    "!{sys.executable} -m pip install keras_metrics\n",
    "!{sys.executable} -m pip install h5py'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading CVS file onto pandas dataframe, a tabular data structure.\n",
    "\n",
    "Printing first few rows, the shape of data (rows, column), the data types, and some basic analysis of the numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col_names = [\n",
    "    'age', \n",
    "    'workclass', \n",
    "    'fnlwgt', \n",
    "    'education', \n",
    "    'education-num', \n",
    "    'marital-status', \n",
    "    'occupation', \n",
    "    'relationship',\n",
    "    'race', \n",
    "    'sex', \n",
    "    'capital-gain', \n",
    "    'capital-loss', \n",
    "    'hours-per-week', \n",
    "    'native-country', \n",
    "    'income',\n",
    "]\n",
    "\n",
    "df = pd.read_csv('data/census_income.csv', names=col_names, skipinitialspace=True, na_values=['?'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the data description available at https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names, \n",
    "feature 'fnlwgt' does not add any useful information, so it is best to remove.\n",
    "\n",
    "Feature \"education\" is also redundant because \"education-num\" encodes the same information with numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.drop('fnlwgt', axis=1)\n",
    "df = df.drop('education', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether numeric features are correlated. Since they are not, all of them bring new relevant information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='Blues', linecolor='white', linewidths=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot data to understand it, determine whether any features look strongly correlated with high or low income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(x = df['income'], hue = df['education-num'], palette = 'rainbow', edgecolor = [(0,0,0), (0,0,0)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "sns.countplot(y = df['income'], hue = df['sex'], palette = 'Set1', edgecolor = [(0,0,0), (0,0,0)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "sns.countplot(y = df['income'], hue = df['marital-status'], palette = 'Set1', edgecolor = [(0,0,0), (0,0,0)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "sns.countplot(y = df['income'], hue = df['relationship'], palette = 'Set1', edgecolor = [(0,0,0), (0,0,0)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(y = df['income'], hue = df['occupation'], palette = 'tab20', edgecolor = [(0,0,0), (0,0,0)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "sns.countplot(y = df['income'], hue = df['race'], palette = 'Set1', edgecolor = [(0,0,0), (0,0,0)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "sns.countplot(y = df['income'], hue = df['workclass'], palette = 'Set1', edgecolor = [(0,0,0), (0,0,0)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxenplot(x='income', y='age', data=df, hue='sex', palette = 'prism')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with NULLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['native-country'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['native-country'].fillna('United-States', inplace=True)\n",
    "df[df['native-country'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['workclass'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['workclass'].fillna('Private', inplace=True)\n",
    "df[df['workclass'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['occupation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df[pd.isnull(df).any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tdf = pd.get_dummies(df, columns=[\n",
    "    'workclass', \n",
    "    'marital-status', \n",
    "    'occupation', \n",
    "    'relationship', \n",
    "    'race', \n",
    "    'sex',\n",
    "    'native-country',\n",
    "], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tdf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataframe into features (X) and labels(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = tdf.drop('income', axis=1)\n",
    "y = tdf['income']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put aside 20% of features to test, train with remaining 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our problem is we have 80 variables to predict a single one. Many of these variables will have absolutely no impact on the income of our citizens, but will add a lot of useless noise to the regression, and may even make it not converge.\n",
    "\n",
    "We use a little trick called SelectKBest that will give us the k=30 best features based on a standard statistical score. In this case, we use Pearson's Chi Squared test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_select = SelectKBest(chi2, k=30)\n",
    "feature_select.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the surviving features, along with their scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uni_features = list(zip(feature_select.scores_, X_train.columns))\n",
    "sorted(uni_features, reverse=True)[0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "joblib.dump(list(X.columns), 'models/columns.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_metrics(classifier):\n",
    "    prediction = classifier.predict(X_test)\n",
    "    print('Accuracy is: %0.3f' % accuracy_score(y_test, prediction))\n",
    "    print('F1 Score is: %0.3f' % f1_score(y_test, prediction, pos_label=\">50K\"))\n",
    "    print(classification_report(y_test, prediction))\n",
    "    sns.heatmap(confusion_matrix(y_test, prediction), annot=True, fmt=\"d\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_roc_curve(classifier, classifier_name):\n",
    "    try:\n",
    "        prob_pos = classifier.predict_proba(X_test)[:,1]\n",
    "    except:\n",
    "        prob_pos = classifier.decision_function(X_test)\n",
    "        prob_pos = (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n",
    "        print('Probabilities estimated from decision function!')\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, prob_pos, pos_label=\">50K\")\n",
    "    plt.figure()\n",
    "    roc_score = roc_auc_score(y_test, classifier.predict(X_test)==\">50K\")\n",
    "    plt.plot(fpr, tpr, label='%s (area = %0.3f)' % (classifier_name, roc_score))\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate = 1 - Specificity')\n",
    "    plt.ylabel('True Positive Rate = Recall')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('img/ROC_' + classifier_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_cap_curve(classifier, classifier_name):\n",
    "    from scipy import integrate\n",
    "    try:\n",
    "        prob_pos = classifier.predict_proba(X_test)[:,1]\n",
    "    except:\n",
    "        prob_pos = classifier.decision_function(X_test)\n",
    "        prob_pos = (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n",
    "        print('Probabilities estimated from decision function!')\n",
    "    y_values = y_test==\">50K\"\n",
    "    num_pos_obs = np.sum(y_values)\n",
    "    num_count = len(y_values)\n",
    "    rate_pos_obs = float(num_pos_obs) / float(num_count)\n",
    "    ideal = pd.DataFrame({'x':[0,rate_pos_obs,1],'y':[0,1,1]})\n",
    "    xx = np.arange(num_count) / float(num_count - 1)\n",
    "    \n",
    "    y_cap = np.c_[y_values,prob_pos]\n",
    "    y_cap_df_s = pd.DataFrame(data=y_cap)\n",
    "    y_cap_df_s = y_cap_df_s.sort_values([1], ascending=False).reset_index(level = y_cap_df_s.index.names, drop=True)\n",
    "    \n",
    "    #print(y_cap_df_s.head(20))\n",
    "    \n",
    "    yy = np.cumsum(y_cap_df_s[0]) / float(num_pos_obs)\n",
    "    yy = np.append([0], yy[0:num_count-1]) #add the first curve point (0,0) : for xx=0 we have yy=0\n",
    "    \n",
    "    percent = 0.5\n",
    "    row_index = int(np.trunc(num_count * percent))\n",
    "    \n",
    "    val_y1 = yy[row_index]\n",
    "    val_y2 = yy[row_index+1]\n",
    "    if val_y1 == val_y2:\n",
    "        val = val_y1*1.0\n",
    "    else:\n",
    "        val_x1 = xx[row_index]\n",
    "        val_x2 = xx[row_index+1]\n",
    "        val = val_y1 + ((val_x2 - percent)/(val_x2 - val_x1))*(val_y2 - val_y1)\n",
    "    \n",
    "    sigma_ideal = 1 * xx[num_pos_obs - 1 ] / 2 + (xx[num_count - 1] - xx[num_pos_obs]) * 1\n",
    "    sigma_model = integrate.simps(yy,xx)\n",
    "    sigma_random = integrate.simps(xx,xx)\n",
    "    \n",
    "    ar_value = (sigma_model - sigma_random) / (sigma_ideal - sigma_random)\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows = 1, ncols = 1)\n",
    "    ax.plot(ideal['x'],ideal['y'], color='dimgrey', label='Perfect Model')\n",
    "    ax.plot(xx,yy, color='red', label=\"%s (AR = %0.3f)\" % (classifier_name, ar_value))\n",
    "    ax.plot(xx,xx, color='blue', label='Random Model')\n",
    "    ax.plot([percent, percent], [0.0, val], color='green', linestyle='--', linewidth=1)\n",
    "    ax.plot([0, percent], [val, val], color='green', linestyle='--', linewidth=1, label='%0.3f%% of positive obs at %0.1f%%' % (val*100, percent*100))\n",
    "    \n",
    "    plt.xlim(-0.05, 1.05)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.title(\"Cumulative accuracy profile\")\n",
    "    plt.xlabel('% of the data')\n",
    "    plt.ylabel('% of positive obs')\n",
    "    plt.legend()\n",
    "    plt.savefig('img/CAP_' + classifier_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def classify_and_plot(classifier):\n",
    "    classifier_name = type(classifier).__name__\n",
    "    if classifier_name == 'Pipeline':\n",
    "        classifier_name = classifier.steps[-1][0]\n",
    "    classifier.fit(X_train, y_train)\n",
    "    plot_metrics(classifier)\n",
    "    plot_roc_curve(classifier, classifier_name)\n",
    "    plot_cap_curve(classifier, classifier_name)\n",
    "    joblib.dump(classifier, 'models/' + classifier_name + '.pkl')\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic Data Science Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nearest Neighbours Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "pipeline = Pipeline([\n",
    "    ('select', feature_select),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('KNeighborsClassifier', KNeighborsClassifier()),\n",
    "])\n",
    "classify_and_plot(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "pipeline = Pipeline([\n",
    "    ('select', feature_select),\n",
    "    ('LogisticRegression', LogisticRegression(solver='liblinear')),\n",
    "])\n",
    "classify_and_plot(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "pipeline = Pipeline([\n",
    "    ('select', feature_select),\n",
    "    ('GaussianNB_30', GaussianNB(priors=[0.75, 0.25])),\n",
    "])\n",
    "classify_and_plot(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "pipeline = Pipeline([\n",
    "    ('GaussianNB_80', GaussianNB(priors=[0.75, 0.25])),\n",
    "])\n",
    "classify_and_plot(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "pipeline = Pipeline([\n",
    "    ('select', feature_select),\n",
    "    ('DecisionTreeClassifier', DecisionTreeClassifier()),\n",
    "])\n",
    "classify_and_plot(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If machine has Graphviz (https://www.graphviz.org/) installed, uncomment to render the decision tree as a PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''#!{sys.executable} -m pip install --quiet pydotplus\n",
    "from sklearn.externals.six import StringIO  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "dot_data = StringIO()\n",
    "export_graphviz(pipeline.steps[-1][1], out_file=dot_data, filled=True, rounded=True, special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "graph.write_pdf(\"models/tree.pdf\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "pipeline = Pipeline([\n",
    "    ('select', feature_select),\n",
    "    ('RandomForestClassifier', RandomForestClassifier(n_estimators=20)),\n",
    "])\n",
    "classify_and_plot(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "pipeline = Pipeline([\n",
    "    ('select', feature_select),\n",
    "    ('AdaBoostClassifier', AdaBoostClassifier(n_estimators=100)),\n",
    "])\n",
    "classify_and_plot(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "pipeline = Pipeline([\n",
    "    ('select', feature_select),\n",
    "    ('XGBClassifier', XGBClassifier(max_depth=100)),\n",
    "])\n",
    "classify_and_plot(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Microsoft LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lightgbm.sklearn import LGBMClassifier\n",
    "pipeline = Pipeline([\n",
    "    ('select', feature_select),\n",
    "    ('LightGBM', LGBMClassifier()),\n",
    "])\n",
    "classify_and_plot(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "pipeline = Pipeline([\n",
    "    ('select', feature_select),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('LinearSVC', LinearSVC(C=0.2)),\n",
    "])\n",
    "classify_and_plot(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC with Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "pipeline = Pipeline([\n",
    "    ('select', feature_select),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('SVC_poly', SVC(C=1.0, kernel='poly', gamma='scale')),\n",
    "])\n",
    "classify_and_plot(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC with Radial Basis Function Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "pipeline = Pipeline([\n",
    "    ('select', feature_select),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('SVC_rbf', SVC(C=1.0, kernel='rbf', gamma='scale')),\n",
    "])\n",
    "classify_and_plot(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import keras_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully connected layers:\n",
    "- 80 inputs\n",
    "- Hidden layer with 40 neurons, ReLU as activation, and dropout rate of 10%\n",
    "- Hidden layer with 20 neurons, ReLU as activation, and dropout rate of 10%\n",
    "- 1 single output with sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(40, kernel_initializer='normal', activation='relu', input_dim=X_train.shape[1]))\n",
    "    #model.add(LeakyReLU(alpha=0.001))\n",
    "    model.add(Dropout(rate=0.1))\n",
    "    model.add(Dense(20, kernel_initializer='normal', activation='relu'))\n",
    "    #model.add(LeakyReLU(alpha=0.001))\n",
    "    model.add(Dropout(rate=0.1))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\n",
    "        'accuracy', \n",
    "        #keras_metrics.precision(), \n",
    "        #keras_metrics.recall()\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features must be scaled by subtracting the mean and scaling to unit variance.\n",
    "\n",
    "Absolute maximum of 250 training epochs, with 2 callbacks:\n",
    "1. Monitor loss on validation test set (val_loss), and stop training once it stops improving\n",
    "2. Persist to disk the model state that yielded the best val_loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''checkpoint = ModelCheckpoint('models/best_model.hdf5', monitor='val_acc',\n",
    "                             mode='max', verbose=0, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_acc', patience=15, verbose=1, mode='max',\n",
    "                          restore_best_weights=True)'''\n",
    "\n",
    "checkpoint = ModelCheckpoint('models/best_model.hdf5', monitor='val_loss',\n",
    "                             mode='min', verbose=0, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=50, verbose=1, mode='min',\n",
    "                          restore_best_weights=True)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('KerasClassifier', KerasClassifier(build_fn=create_model, epochs=250, batch_size=5, verbose=1, \n",
    "                            callbacks=[checkpoint, early_stop], \n",
    "                            validation_data=(X_test, y_test==\">50K\"))),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classify_and_plot(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "notify_time": "10"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
